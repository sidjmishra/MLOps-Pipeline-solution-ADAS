import os
import json
import time
import joblib
import traceback
import pandas as pd
from pymongo import MongoClient
from datetime import datetime, timedelta

from data_preparation import fetch_data_from_mongodb, prepare_data
from model_training import train_model, evaluate_model
from model_versioning import save_model, get_latest_model_version, load_model

from evidently.report import Report
from evidently.metric_preset import DataDriftPreset, ClassificationPreset


MONGO_URI = "MONGO-URL"
DATABASE_NAME = "mlops_pipeline"
COLLECTION_NAME = "sensor_readings"

RETRAIN_INTERVAL_SECONDS = 3600
DRIFT_CHECK_INTERVAL_SECONDS = 900
MIN_DATA_FOR_TRAINING = 500
DRIFT_THRESHOLD_P_VALUE = 0.05

BASELINE_DATA_PATH = "baseline_data.pkl"
LAST_CHECK_TIME_FILE = "last_drift_check_time.txt"

preprocessor_loaded = None 

def get_db_collection():
    client = MongoClient(MONGO_URI)
    db = client[DATABASE_NAME]
    return client, db[COLLECTION_NAME]

def get_last_check_timestamp():
    if os.path.exists(LAST_CHECK_TIME_FILE):
        with open(LAST_CHECK_TIME_FILE, 'r') as f:
            timestamp_str = f.read().strip()
            if timestamp_str:
                return datetime.fromisoformat(timestamp_str)
    return datetime.min

def set_last_check_timestamp(timestamp: datetime):
    with open(LAST_CHECK_TIME_FILE, 'w') as f:
        f.write(timestamp.isoformat())

def get_production_model_metrics(version_name):
    metrics_path = os.path.join("models", f"{version_name}_metrics.json")
    if os.path.exists(metrics_path):
        with open(metrics_path, 'r') as f:
            return json.load(f)
    return {}

def activate_model_in_api(version_name: str):
    active_model_file = os.path.join("models", "active_model_version.txt")
    try:
        with open(active_model_file, 'w') as f:
            f.write(version_name)
        print(f"Simulated API activation: Set active model to '{version_name}'.")
    except Exception as e:
        print(f"Error simulating model activation: {e}")

def run_mlops_pipeline():
    global preprocessor_loaded

    last_retrain_time = datetime.now() - timedelta(days=365)
    last_drift_check_time = get_last_check_timestamp()

    client, collection = get_db_collection()
    initial_data_cursor = collection.find({}).limit(1000)
    initial_data = list(initial_data_cursor)
    client.close()
    
    if not initial_data:
        print("Waiting for initial data to be collected in MongoDB for baseline.")
        print("Please run the data generation script first.")
        return

    baseline_df = pd.DataFrame(initial_data).drop(columns=['_id', 'record_id', 'timestamp'], errors='ignore')
    baseline_df['intervention_needed'] = baseline_df['intervention_needed'].astype(int)
    
    joblib.dump(baseline_df, BASELINE_DATA_PATH)
    print(f"Baseline data saved to {BASELINE_DATA_PATH}")


    if not get_latest_model_version():
        print("No model found. Performing initial training and deployment.")
        df = fetch_data_from_mongodb()
        if not df.empty and df.shape[0] >= MIN_DATA_FOR_TRAINING:
            X_train, X_test, y_train, y_test, initial_preprocessor = prepare_data(df)
            if X_train is not None:
                model = train_model(X_train, y_train)
                if model:
                    metrics = evaluate_model(model, X_test, y_test)
                    version_name, _ = save_model(model, metrics)
                    activate_model_in_api(version_name)
                    preprocessor_loaded = initial_preprocessor
                    print("Initial model trained and deployed.")
                    last_retrain_time = datetime.now()
        else:
            print("Not enough initial data to train a model. Waiting for more data.")
    
    if preprocessor_loaded is None and os.path.exists('preprocessor.pkl'):
        preprocessor_loaded = joblib.load('preprocessor.pkl')
        print("Preprocessor loaded from preprocessor.pkl for ongoing monitoring.")
    elif preprocessor_loaded is None:
        print("Warning: preprocessor.pkl not found and not generated by initial training. Model monitoring might be limited.")


    while True:
        current_time = datetime.now()
        
        drift_detected = False

        if (current_time - last_drift_check_time).total_seconds() >= DRIFT_CHECK_INTERVAL_SECONDS:
            print(f"\n--- Checking for Data and Model Drift at {current_time.strftime('%Y-%m-%d %H:%M:%S')} ---")
            
            client, collection = get_db_collection()
            recent_data_cursor = collection.find({
                "timestamp": {"$gte": last_drift_check_time}
            }).limit(2000)
            current_raw_data_df = pd.DataFrame(list(recent_data_cursor))
            client.close()

            if current_raw_data_df.empty or 'intervention_needed' not in current_raw_data_df.columns:
                print("Not enough recent data for drift detection or target column missing.")
            else:
                current_raw_data_df = current_raw_data_df.drop(columns=['_id', 'record_id', 'timestamp'], errors='ignore')
                current_raw_data_df['intervention_needed'] = current_raw_data_df['intervention_needed'].astype(int)

                baseline_df = joblib.load(BASELINE_DATA_PATH)
                
                active_version = get_latest_model_version()
                active_model, _ = (load_model(active_version) if active_version else (None, None))
                
                if active_model and preprocessor_loaded:
                    try:
                        features_for_prediction = current_raw_data_df.drop('intervention_needed', axis=1, errors='ignore')
                        
                        processed_features = preprocessor_loaded.transform(features_for_prediction)
                        
                        if hasattr(processed_features, 'toarray'):
                            processed_features = processed_features.toarray()

                        y_pred = active_model.predict(processed_features)
                        y_proba = active_model.predict_proba(processed_features)[:, 1]

                        current_data_for_report = current_raw_data_df.copy()
                        current_data_for_report['target'] = current_data_for_report['intervention_needed']
                        current_data_for_report['prediction'] = y_pred
                        current_data_for_report['prediction_proba'] = y_proba

                        # --- Generate predictions and probabilities for baseline data as well ---
                        baseline_df_for_report = baseline_df.copy()
                        baseline_df_for_report['target'] = baseline_df_for_report['intervention_needed']
                        
                        # Prepare features from baseline_df for prediction
                        baseline_features_for_prediction = baseline_df_for_report.drop('target', axis=1, errors='ignore')
                        processed_baseline_features = preprocessor_loaded.transform(baseline_features_for_prediction)
                        
                        if hasattr(processed_baseline_features, 'toarray'):
                            processed_baseline_features = processed_baseline_features.toarray()

                        y_pred_baseline = active_model.predict(processed_baseline_features)
                        y_proba_baseline = active_model.predict_proba(processed_baseline_features)[:, 1]

                        baseline_df_for_report['prediction'] = y_pred_baseline
                        baseline_df_for_report['prediction_proba'] = y_proba_baseline
                        # --- End of baseline data preparation for report ---

                        # --- Generate Data Profiling Report ---
                        """
                        print("  Generating data profiling report for current data...")
                        profile_report = ProfileReport(current_data_for_report, title="Current Data Profile", explorative=True, lazy=False)
                        profile_report_path = "current_data_profile.html"
                        profile_report.to_file(profile_report_path)
                        print(f"  Data profiling report saved to {profile_report_path}")
                        """
                        # --- End Data Profiling Report ---

                        drift_report = Report(metrics=[
                            DataDriftPreset(),
                            ClassificationPreset()
                        ])
                        
                        drift_report.run(
                            reference_data=baseline_df_for_report,
                            current_data=current_data_for_report,
                            # column_mapping = {
                            #     'target': 'intervention_needed',
                            #     'prediction': 'prediction',
                            #     'prediction_probas': 'prediction_proba',
                            #     'numerical_features': [col for col in baseline_df_for_report.columns if baseline_df_for_report[col].dtype in ['int64', 'float64'] and col not in ['intervention_needed', 'prediction', 'prediction_proba']],
                            #     'categorical_features': [col for col in baseline_df_for_report.columns if baseline_df_for_report[col].dtype == 'object' and col not in ['intervention_needed', 'prediction', 'prediction_proba']]
                            # }
                        )
                        
                        report_html_path = "drift_report.html"
                        drift_report.save_html(report_html_path)
                        print(f"Drift report saved to {report_html_path}")

                        data_drift_result = drift_report.as_dict()['metrics'][0]['result']
                        if data_drift_result.get('dataset_drift'):
                            print("  Dataset drift detected!")
                            drift_detected = True

                        classification_result = drift_report.as_dict()['metrics'][1]['result']
                        current_accuracy = drift_report.as_dict()['metrics'][2]['result']['current']['accuracy']
                        current_f1 = drift_report.as_dict()['metrics'][2]['result']['current']['f1']
                        
                        print(f"  Current Accuracy: {current_accuracy:.4f}")
                        print(f"  Current F1 Score: {current_f1:.4f}")

                        if active_version:
                            current_active_model_metrics = get_production_model_metrics(active_version)
                            baseline_f1 = current_active_model_metrics.get('f1_score', 0)
                            baseline_accuracy = current_active_model_metrics.get('accuracy', 0)

                            if current_f1 < baseline_f1 * 0.95:
                                print(f"  Model F1 score dropped from {baseline_f1:.4f} to {current_f1:.4f}. Performance degradation detected!")
                                drift_detected = True
                            elif current_accuracy < baseline_accuracy * 0.95:
                                print(f"  Model Accuracy dropped from {baseline_accuracy:.4f} to {current_accuracy:.4f}. Performance degradation detected!")
                                drift_detected = True
                        
                        if drift_detected:
                            print("  Drift detected, considering retraining.")
                        else:
                            print("  No significant drift or performance degradation detected.")
                            
                    except Exception as e:
                        drift_detected = False

                else:
                    print("Active model or preprocessor not loaded. Cannot run full drift detection.")

            set_last_check_timestamp(current_time)

        if (current_time - last_retrain_time).total_seconds() >= RETRAIN_INTERVAL_SECONDS or drift_detected:
            print(f"\n--- Retraining Model at {current_time.strftime('%Y-%m-%d %H:%M:%S')} ---")
            
            df = fetch_data_from_mongodb()
            
            if not df.empty and df.shape[0] >= MIN_DATA_FOR_TRAINING:
                X_train, X_test, y_train, y_test, new_preprocessor = prepare_data(df)
                if X_train is not None:
                    model = train_model(X_train, y_train)
                    if model:
                        new_metrics = evaluate_model(model, X_test, y_test)
                        
                        active_version = get_latest_model_version()
                        if active_version:
                            current_metrics = get_production_model_metrics(active_version)
                            
                            if new_metrics['f1_score'] > current_metrics.get('f1_score', 0):
                                print(f"New model (F1: {new_metrics['f1_score']:.4f}) is better than current (F1: {current_metrics.get('f1_score', 0):.4f}).")
                                version_name, _ = save_model(model, new_metrics)
                                activate_model_in_api(version_name)
                                preprocessor_loaded = new_preprocessor
                                print(f"New model version '{version_name}' deployed!")
                            else:
                                print(f"New model (F1: {new_metrics['f1_score']:.4f}) is NOT better than current (F1: {current_metrics.get('f1_score', 0):.4f}). Keeping current model.")
                        else:
                            print("No active model found. Deploying the newly trained model.")
                            version_name, _ = save_model(model, new_metrics)
                            activate_model_in_api(version_name)
                            preprocessor_loaded = new_preprocessor
                            print(f"Initial model version '{version_name}' deployed!")
                
                last_retrain_time = current_time

            else:
                print(f"Not enough data for retraining. Need at least {MIN_DATA_FOR_TRAINING} records.")

        time.sleep(60)

if __name__ == "__main__":
    print("Starting MLOps Orchestrator...")
    try:
        run_mlops_pipeline()
    except KeyboardInterrupt:
        print("\nMLOps Orchestrator stopped by user.")
    except Exception as e:
        print(f"An error occurred in the orchestrator: {e}")
